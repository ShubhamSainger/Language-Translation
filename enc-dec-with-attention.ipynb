{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11276356,"sourceType":"datasetVersion","datasetId":7049651}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tokenizers\nimport torch\nfrom torch import nn\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:54.952298Z","iopub.execute_input":"2025-04-22T13:17:54.952587Z","iopub.status.idle":"2025-04-22T13:17:59.239715Z","shell.execute_reply.started":"2025-04-22T13:17:54.952536Z","shell.execute_reply":"2025-04-22T13:17:59.238776Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"data = pd.read_table(\"/kaggle/input/language-translation/Sentence pairs in English-Hindi - 2025-04-02.tsv\", header=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.240860Z","iopub.execute_input":"2025-04-22T13:17:59.241308Z","iopub.status.idle":"2025-04-22T13:17:59.309907Z","shell.execute_reply.started":"2025-04-22T13:17:59.241276Z","shell.execute_reply":"2025-04-22T13:17:59.309022Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\ndata.columns = [\"Eng-ID\",\"Eng\",\"Hin-ID\",\"Hin\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.311883Z","iopub.execute_input":"2025-04-22T13:17:59.312198Z","iopub.status.idle":"2025-04-22T13:17:59.316001Z","shell.execute_reply.started":"2025-04-22T13:17:59.312176Z","shell.execute_reply":"2025-04-22T13:17:59.315217Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data = data.sample(data.shape[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.317457Z","iopub.execute_input":"2025-04-22T13:17:59.317966Z","iopub.status.idle":"2025-04-22T13:17:59.335343Z","shell.execute_reply.started":"2025-04-22T13:17:59.317929Z","shell.execute_reply":"2025-04-22T13:17:59.334791Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"English_data = data.loc[:,[\"Eng-ID\",\"Eng\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.336111Z","iopub.execute_input":"2025-04-22T13:17:59.336402Z","iopub.status.idle":"2025-04-22T13:17:59.355377Z","shell.execute_reply.started":"2025-04-22T13:17:59.336369Z","shell.execute_reply":"2025-04-22T13:17:59.354704Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"Hindi_data = data.loc[:,[\"Hin-ID\",\"Hin\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.356212Z","iopub.execute_input":"2025-04-22T13:17:59.356447Z","iopub.status.idle":"2025-04-22T13:17:59.369941Z","shell.execute_reply.started":"2025-04-22T13:17:59.356427Z","shell.execute_reply":"2025-04-22T13:17:59.369216Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"hin_itr = Hindi_data[\"Hin\"].to_list()\neng_itr = English_data[\"Eng\"].to_list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.370665Z","iopub.execute_input":"2025-04-22T13:17:59.370850Z","iopub.status.idle":"2025-04-22T13:17:59.385049Z","shell.execute_reply.started":"2025-04-22T13:17:59.370833Z","shell.execute_reply":"2025-04-22T13:17:59.384410Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from tokenizers import Tokenizer\n\nfrom tokenizers import normalizers\nfrom tokenizers.normalizers import NFKC, Lowercase\n\nfrom tokenizers import pre_tokenizers\nfrom tokenizers.pre_tokenizers import Whitespace, Punctuation, Digits, BertPreTokenizer\n\nfrom tokenizers.processors import TemplateProcessing\n\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.387153Z","iopub.execute_input":"2025-04-22T13:17:59.387377Z","iopub.status.idle":"2025-04-22T13:17:59.401882Z","shell.execute_reply.started":"2025-04-22T13:17:59.387356Z","shell.execute_reply":"2025-04-22T13:17:59.401214Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n\n\n\nhin_tokenizer = Tokenizer(BPE(unk_token = \"[UNK]\"))\n\nhin_tokenizer.normalizer = normalizers.Sequence([NFKC(), Lowercase()])\n\nhin_tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits(individual_digits=True)])\n\nhin_tokenizer.post_processor = TemplateProcessing(\n    single = \"[SOS] $A [EOS]\",\n    special_tokens = [\n        (\"[SOS]\",1),\n        (\"[EOS]\",2),\n        (\"[PAD]\",0)\n    ]\n)\n\ntrainer = BpeTrainer( special_tokens = [\"[PAD]\", \"[SOS]\" , \"[EOS]\", \"[UNK]\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.403315Z","iopub.execute_input":"2025-04-22T13:17:59.403554Z","iopub.status.idle":"2025-04-22T13:17:59.422754Z","shell.execute_reply.started":"2025-04-22T13:17:59.403535Z","shell.execute_reply":"2025-04-22T13:17:59.421817Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"hin_tokenizer.train_from_iterator(hin_itr,trainer)\n\nhin_tokenizer.enable_padding()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.423534Z","iopub.execute_input":"2025-04-22T13:17:59.423841Z","iopub.status.idle":"2025-04-22T13:17:59.790448Z","shell.execute_reply.started":"2025-04-22T13:17:59.423814Z","shell.execute_reply":"2025-04-22T13:17:59.789782Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"hin_tokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.791152Z","iopub.execute_input":"2025-04-22T13:17:59.791425Z","iopub.status.idle":"2025-04-22T13:17:59.800460Z","shell.execute_reply.started":"2025-04-22T13:17:59.791389Z","shell.execute_reply":"2025-04-22T13:17:59.799688Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"10463"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"hin_encode = hin_tokenizer.encode_batch(hin_itr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.801398Z","iopub.execute_input":"2025-04-22T13:17:59.801770Z","iopub.status.idle":"2025-04-22T13:18:00.179322Z","shell.execute_reply.started":"2025-04-22T13:17:59.801714Z","shell.execute_reply":"2025-04-22T13:18:00.178648Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"hin_vocab_len = hin_tokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.180069Z","iopub.execute_input":"2025-04-22T13:18:00.180349Z","iopub.status.idle":"2025-04-22T13:18:00.186668Z","shell.execute_reply.started":"2025-04-22T13:18:00.180321Z","shell.execute_reply":"2025-04-22T13:18:00.185813Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"eng_tokenizer = Tokenizer(BPE(unk_token = \"[UNK]\"))\n\neng_tokenizer.normalizer = normalizers.Sequence([NFKC(), Lowercase()])\n\neng_tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits(individual_digits=True)])\n\neng_tokenizer.post_processor = TemplateProcessing(\n    single = \"[SOS] $A [EOS]\",\n    special_tokens = [\n        (\"[SOS]\",1),\n        (\"[EOS]\",2),\n        (\"[PAD]\",0)\n    ]\n)\n\ntrainer = BpeTrainer( special_tokens = [\"[PAD]\", \"[SOS]\" , \"[EOS]\", \"[UNK]\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.187803Z","iopub.execute_input":"2025-04-22T13:18:00.188103Z","iopub.status.idle":"2025-04-22T13:18:00.202311Z","shell.execute_reply.started":"2025-04-22T13:18:00.188070Z","shell.execute_reply":"2025-04-22T13:18:00.201660Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"eng_tokenizer.train_from_iterator(eng_itr,trainer)\neng_tokenizer.enable_padding()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.203199Z","iopub.execute_input":"2025-04-22T13:18:00.203486Z","iopub.status.idle":"2025-04-22T13:18:00.498826Z","shell.execute_reply.started":"2025-04-22T13:18:00.203464Z","shell.execute_reply":"2025-04-22T13:18:00.497902Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"eng_vocab_len = eng_tokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.499745Z","iopub.execute_input":"2025-04-22T13:18:00.500036Z","iopub.status.idle":"2025-04-22T13:18:00.506314Z","shell.execute_reply.started":"2025-04-22T13:18:00.500010Z","shell.execute_reply":"2025-04-22T13:18:00.505474Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"eng_encode = eng_tokenizer.encode_batch(eng_itr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.507225Z","iopub.execute_input":"2025-04-22T13:18:00.507434Z","iopub.status.idle":"2025-04-22T13:18:00.843759Z","shell.execute_reply.started":"2025-04-22T13:18:00.507416Z","shell.execute_reply":"2025-04-22T13:18:00.842699Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"hind_padded_size = len(hin_encode[0])\neng_padded_size = len(eng_encode[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.844558Z","iopub.execute_input":"2025-04-22T13:18:00.845125Z","iopub.status.idle":"2025-04-22T13:18:00.851360Z","shell.execute_reply.started":"2025-04-22T13:18:00.845030Z","shell.execute_reply":"2025-04-22T13:18:00.850232Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"training_size = int(data.shape[0] * 0.7)\n# testing_size = data.shape[0] - training_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.852475Z","iopub.execute_input":"2025-04-22T13:18:00.852822Z","iopub.status.idle":"2025-04-22T13:18:00.868823Z","shell.execute_reply.started":"2025-04-22T13:18:00.852789Z","shell.execute_reply":"2025-04-22T13:18:00.868142Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"shuffeled_index = np.random.permutation(np.arange(0,data.shape[0]))\ntraining_eng_data = np.array(eng_encode)[shuffeled_index[:training_size]]\ntraining_hin_data = np.array(hin_encode)[shuffeled_index[:training_size]]\n\n\ntesting_eng_data = np.array(eng_encode)[shuffeled_index[training_size:]]\ntesting_hin_data = np.array(hin_encode)[shuffeled_index[training_size:]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.869555Z","iopub.execute_input":"2025-04-22T13:18:00.869838Z","iopub.status.idle":"2025-04-22T13:18:00.960246Z","shell.execute_reply.started":"2025-04-22T13:18:00.869818Z","shell.execute_reply":"2025-04-22T13:18:00.959516Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eng_tokenizer.decode_batch([x.ids for x in testing_eng_data[:10]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.961026Z","iopub.execute_input":"2025-04-22T13:18:00.961254Z","iopub.status.idle":"2025-04-22T13:18:00.967243Z","shell.execute_reply.started":"2025-04-22T13:18:00.961235Z","shell.execute_reply":"2025-04-22T13:18:00.966630Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['is that a cat ?',\n \"they ' re using you .\",\n 'he likes playing soccer .',\n 'they dug here and there for treasure .',\n \"you ' re doing very well . keep it up .\",\n 'she has lived there for seven years .',\n \"you often ask questions i can ' t answer .\",\n 'we were worried about you .',\n 'let me know if you find anything .',\n 'just wait a minute .']"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"hin_tokenizer.decode_batch([x.ids for x in testing_hin_data[:10]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.968047Z","iopub.execute_input":"2025-04-22T13:18:00.968291Z","iopub.status.idle":"2025-04-22T13:18:00.984973Z","shell.execute_reply.started":"2025-04-22T13:18:00.968272Z","shell.execute_reply":"2025-04-22T13:18:00.984162Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['वह बिल्ली है क्या ?',\n 'वे तुम्हारा इस्तेमाल कर रहे हैं ।',\n 'उसको फ़ुटबॉल खेलना अच्छा लगता है ।',\n 'उन्होंने ख़ज़ाना ढूँढने के लिए यहाँ - वहाँ खोदा ।',\n 'तुम अच्छा काम कर रहे हो । ऐसे ही करते रहो ।',\n 'वह वहाँ सात साल रही है ।',\n 'आप अकसर ऐसे सवाल पूछते हैं जिनका मैं जवाब नहीं दे सकती ।',\n 'हमें आपकी चिंता लगी हुई है ।',\n 'कुछ मिल जाये तो मुझे बता देना ।',\n 'बस एक मिनट रुक ।']"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Class to create a encoder which will take input sequence and give the hidden states at each time step\n\nclass encoder(nn.Module):\n    def __init__(self):        \n        super().__init__()\n        self.embedding_layer = nn.Embedding(eng_vocab_len, 620)\n        self.rnn1 = nn.LSTM(620,1000, batch_first =True, bidirectional = True)\n        self.dropout = nn.Dropout(p = 0.3) \n\n    def forward(self,input_batch):\n        x = self.embedding_layer(input_batch)\n        x, (h,c) = self.rnn1(x)\n        x = self.dropout(x)\n        return (x, (h,c))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.988641Z","iopub.execute_input":"2025-04-22T13:18:00.988857Z","iopub.status.idle":"2025-04-22T13:18:01.000308Z","shell.execute_reply.started":"2025-04-22T13:18:00.988838Z","shell.execute_reply":"2025-04-22T13:18:00.999595Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Class to calculate the context vector for the current time step by taking the hidden state of the decoder and all the hidden states given by the \n# encoder\n\nclass attention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dense_1 = nn.Linear(2000,1000)\n        self.dense_2 = nn.Linear(1000,1000)\n        self.dense_3 = nn.Linear(1000,1)\n\n    def forward(self, decoder_hidden_state, encoder_hidden_state):\n\n        x = self.dense_1(encoder_hidden_state)\n        y = self.dense_2(decoder_hidden_state).permute(1,0,2)\n        x = torch.add(x,y)\n        x = nn.functional.tanh(x)\n        x = self.dense_3(x)\n        x = nn.functional.softmax(x, dim = 1)\n        x  = torch.sum(x * encoder_hidden_state, dim = 1)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:01.001673Z","iopub.execute_input":"2025-04-22T13:18:01.001978Z","iopub.status.idle":"2025-04-22T13:18:01.016834Z","shell.execute_reply.started":"2025-04-22T13:18:01.001946Z","shell.execute_reply":"2025-04-22T13:18:01.015656Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# This is the maxout layer explained in the paper and it will project the data in two different vector spaces of same dimension and \n# then calculate the maximum value of a element from both the dimensions.\n\nclass maxout(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Dense_1 = nn.Linear(1000,hin_vocab_len)\n        self.Dense_2 = nn.Linear(1000,hin_vocab_len)\n\n    def forward(self,x):\n        lp_1 = self.Dense_1(x)\n        lp_2 = self.Dense_2(x)\n        lp_1 = torch.cat([lp_1,lp_2], dim = 1)\n        lp_1 = lp_1.max(dim = 1)[0].unsqueeze(1)\n        return lp_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:01.017847Z","iopub.execute_input":"2025-04-22T13:18:01.018127Z","iopub.status.idle":"2025-04-22T13:18:01.036791Z","shell.execute_reply.started":"2025-04-22T13:18:01.018096Z","shell.execute_reply":"2025-04-22T13:18:01.036010Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Class to create a decoder which will take previous prediction, hidden states given by encoder at each time step,\n# it's own previous hidden states and cell states\n\nclass decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(hin_vocab_len,620, padding_idx=0)\n        self.norm_1 = nn.LayerNorm(620 + 2000)\n        self.rnn = nn.LSTM(620 + 2000,1000,batch_first =True)\n        self.norm_2 = nn.LayerNorm(1000)\n        self.dropout= nn.Dropout(p = 0.3)\n        # self.dense = nn.Linear(800,hin_vocab_len)\n        self.maxout = maxout()\n        self.attention = attention()\n\n\n    def forward(self, y_last_pred, encoder_hidden_states, decoder_hidden_state, cell_hidden_state):\n\n        y_last_pred = self.embedding(y_last_pred)\n        context_vector = self.attention(decoder_hidden_state, encoder_hidden_states).unsqueeze(1)\n        y_last_pred = torch.cat((context_vector,y_last_pred),dim=2)\n        y_last_pred = self.norm_1(y_last_pred)\n        x , (hidden, cell) = self.rnn(y_last_pred,(decoder_hidden_state,cell_hidden_state))\n        x = self.norm_2(x)\n        x = self.dropout(x)\n        # x = self.dense(x)\n        x = self.maxout(x)\n        return x, hidden,cell\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:01.037523Z","iopub.execute_input":"2025-04-22T13:18:01.037843Z","iopub.status.idle":"2025-04-22T13:18:01.046281Z","shell.execute_reply.started":"2025-04-22T13:18:01.037814Z","shell.execute_reply":"2025-04-22T13:18:01.045592Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"e = encoder().cuda()\nd = decoder().cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:01.047044Z","iopub.execute_input":"2025-04-22T13:18:01.047276Z","iopub.status.idle":"2025-04-22T13:18:02.015809Z","shell.execute_reply.started":"2025-04-22T13:18:01.047258Z","shell.execute_reply":"2025-04-22T13:18:02.015081Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"loss = nn.CrossEntropyLoss(ignore_index = 0 ).cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:02.016625Z","iopub.execute_input":"2025-04-22T13:18:02.016919Z","iopub.status.idle":"2025-04-22T13:18:02.021389Z","shell.execute_reply.started":"2025-04-22T13:18:02.016892Z","shell.execute_reply":"2025-04-22T13:18:02.020771Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"encoder_bias_parm = []\nencoder_thetha = []\nfor name, weight in list(e.named_parameters()):\n    if \"bias\" in name:\n        encoder_bias_parm.append(weight)\n\n    else:\n        encoder_thetha.append(weight)\n\n\ndecoder_bias_parm = []\ndecoder_theta = []\nfor name, weight in list(d.named_parameters()):\n    if \"bias\" in name:\n        decoder_bias_parm.append(weight)\n\n    else:\n        decoder_theta.append(weight)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:02.038698Z","iopub.execute_input":"2025-04-22T13:18:02.039046Z","iopub.status.idle":"2025-04-22T13:18:02.053933Z","shell.execute_reply.started":"2025-04-22T13:18:02.039014Z","shell.execute_reply":"2025-04-22T13:18:02.053219Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Applying regularization only on parameter vectors not on the bias terms\n\noptim = torch.optim.Adam([{\"params\":encoder_bias_parm + decoder_bias_parm, \"weight_decay\":0},\n                        {\"params\": encoder_thetha + decoder_theta}], weight_decay = 0.005, lr = 0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:02.069710Z","iopub.execute_input":"2025-04-22T13:18:02.069894Z","iopub.status.idle":"2025-04-22T13:18:03.859175Z","shell.execute_reply.started":"2025-04-22T13:18:02.069878Z","shell.execute_reply":"2025-04-22T13:18:03.858486Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Main training loop\n\ndef lang_training(orignal_sequence, target_sequence,val_orignal_sequence, val_target_sequence, batch_size, epochs, encoder, decoder, target_vocab_len,):\n\n    \n    total_sequence = (len(orignal_sequence)//batch_size) * batch_size\n    taget_sequence_length = torch.tensor(orignal_sequence[0].ids).shape[0]\n    average_training_loss = {}\n    orignal_sequence = np.array(orignal_sequence)\n    target_sequence = np.array(target_sequence)\n    \n    for epoch in range(1,epochs):\n        encoder.train()\n        decoder.train()\n        random_index = np.random.permutation(np.arange(0,orignal_sequence.shape[0]))\n        orignal_sequence = orignal_sequence[random_index]\n        target_sequence = target_sequence[random_index]\n        loss_per_batch = []\n        with tqdm(range(0,total_sequence, batch_size)) as tq:\n            for time_step, idx in enumerate(tq):\n        \n                \n                # Collecting inputs and targets.\n                input_ = torch.tensor([x.ids for x in orignal_sequence[idx:idx + batch_size]]).cuda()                \n                output_ = torch.tensor([x.ids for x in target_sequence[idx:idx + batch_size]]).cuda()\n        \n                # Extracting all the hidden states and the cell states from the encoder, using the [SOS] token for a batch.\n                en_out = encoder.forward(input_)\n                \n                hidden = en_out[1][0][1].unsqueeze(0) \n                cell = en_out[1][1][1].unsqueeze(0)\n    \n                en_out = en_out[0]\n            \n                # Assigning 2 np arrays for storing the probabilites of each token and the target token respectively in each timestep.\n                y_p_loss = torch.zeros(68,batch_size,target_vocab_len).cuda()\n                y_t_loss = torch.zeros(68,batch_size).cuda()\n        \n            \n                # Using the final hidden state of the encoder as the initial hidden state of the decoder and generating the y_1_hat \n                y_last_pred, hidden, cell = decoder.forward(output_[:,0].unsqueeze(1), en_out, hidden, cell)\n         \n                y_true = output_[:,1] # Loss will be calculated on target[t+1] index\n                l = 0\n        \n                y_p_loss[0] = y_last_pred.squeeze(1)\n                y_t_loss[0] = y_true\n                \n                # Finding the index of the token which have the max probability\n                y_last_pred = y_last_pred.argmax(dim = 2)\n                \n        \n                # Generating the probabilites of the tokens for further timesteps and storing them in the assigined variables\n                for i in range(1,68):\n                    \n                    y_last_pred,  hidden, cell = decoder.forward(y_last_pred, en_out, hidden, cell)\n                    y_true = output_[:,i+1]            \n                    y_p_loss[i] = y_last_pred.squeeze(1)\n                    y_t_loss[i] = y_true\n                    \n                    y_last_pred = y_last_pred.argmax(dim = 2)\n        \n                # Reshaping the tensors to match the requirement of the CCE loss -> (Batchsize * sequence, classes)\n                y_p_loss=y_p_loss.view(-1, target_vocab_len)\n                y_t_loss=y_t_loss.view(-1).type(torch.long)\n            \n                l = loss(y_p_loss, y_t_loss)\n                optim.zero_grad()\n                l.backward()\n                optim.step()\n                loss_per_batch.append(l.detach().cpu())\n                tq.set_postfix({\"Loss\":torch.tensor(loss_per_batch).mean()})\n\n        # average_training_loss[epoch] = torch.tensor(loss_per_batch).mean()\n        # print(f\"Epoch {epoch}           training_loss {average_training_loss[epoch]} \")\n        \n        val_loss = validation_performance(val_orignal_sequence, val_target_sequence, 50, e,d, target_vocab_len)\n        print(f\"Epoch {epoch}           val_loss {val_loss}\")\n    return average_training_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:03.859898Z","iopub.execute_input":"2025-04-22T13:18:03.860222Z","iopub.status.idle":"2025-04-22T13:18:03.870278Z","shell.execute_reply.started":"2025-04-22T13:18:03.860202Z","shell.execute_reply":"2025-04-22T13:18:03.869421Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Slight modification of main training loop to work on validation data\n\ndef validation_performance(orignal_sequence,target_sequence,batch_size,encoder,decoder,target_vocab_len):\n    encoder.eval()\n    decoder.eval()\n    total_sequence = (len(orignal_sequence)//batch_size) * batch_size\n    taget_sequence_length = torch.tensor(orignal_sequence[0].ids).shape[0]\n    loss_per_batch = []\n    with torch.no_grad():\n        for time_step, idx in enumerate(range(0,total_sequence, batch_size)):\n        \n            # Collecting inputs and targets.\n            input_ = torch.tensor([x.ids for x in orignal_sequence[idx:idx + batch_size]]).cuda()                \n            output_ = torch.tensor([x.ids for x in target_sequence[idx:idx + batch_size]]).cuda()\n        \n            # Extracting all the hidden states and the cell states from the encoder, using the [SOS] token for a batch.\n            en_out = encoder.forward(input_)\n            \n            hidden = en_out[1][0][1].unsqueeze(0) \n            cell = en_out[1][1][1].unsqueeze(0)\n        \n            en_out = en_out[0]\n        \n            # Assigning 2 np arrays for storing the probabilites of each token and the target token respectively in each timestep.\n            y_p_loss = torch.zeros(68,batch_size,target_vocab_len).cuda()\n            y_t_loss = torch.zeros(68,batch_size).cuda()\n        \n        \n            # Using the final hidden state of the encoder as the initial hidden state of the decoder and generating the y_1_hat \n            y_last_pred, hidden, cell = decoder.forward(output_[:,0].unsqueeze(1), en_out, hidden, cell)\n        \n            y_true = output_[:,1] # Loss will be calculated on target[t+1] index\n            l = 0\n            y_p_loss[0] = y_last_pred.squeeze(1)\n            y_t_loss[0] = y_true\n            \n            # Finding the index of the token which have the max probability\n            y_last_pred = y_last_pred.argmax(dim = 2)\n            \n        \n            # Generating the probabilites of the tokens for further timesteps and storing them in the assigined variables\n            for i in range(1,68):\n                \n                y_last_pred,  hidden, cell = decoder.forward(y_last_pred, en_out, hidden, cell)\n                y_true = output_[:,i+1]            \n                y_p_loss[i] = y_last_pred.squeeze(1)\n                y_t_loss[i] = y_true\n                \n                y_last_pred = y_last_pred.argmax(dim = 2)\n        \n            # Reshaping the tensors to match the requirement of the CCE loss -> (Batchsize * sequence, classes)\n            y_p_loss=y_p_loss.view(-1, target_vocab_len)\n            y_t_loss=y_t_loss.view(-1).type(torch.long)\n        \n            l = loss(y_p_loss, y_t_loss)\n            loss_per_batch.append(l.detach().cpu())\n\n    return torch.tensor(loss_per_batch).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:03.899929Z","iopub.execute_input":"2025-04-22T13:18:03.900196Z","iopub.status.idle":"2025-04-22T13:18:03.914684Z","shell.execute_reply.started":"2025-04-22T13:18:03.900164Z","shell.execute_reply":"2025-04-22T13:18:03.913848Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"\nlang_training(eng_encode[:-1500], hin_encode[:-1500],eng_encode[-1500:],hin_encode[-1500:], 180, 30, e,d, hin_vocab_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:03.915663Z","iopub.execute_input":"2025-04-22T13:18:03.915938Z","iopub.status.idle":"2025-04-22T14:48:25.849722Z","shell.execute_reply.started":"2025-04-22T13:18:03.915918Z","shell.execute_reply":"2025-04-22T14:48:25.848872Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 64/64 [02:57<00:00,  2.77s/it, Loss=tensor(5.6287)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1           val_loss 4.960709095001221\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(4.7877)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2           val_loss 4.5693159103393555\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(4.4330)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3           val_loss 4.369600296020508\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(4.2107)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4           val_loss 4.193140029907227\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(4.0409)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5           val_loss 4.084902763366699\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.9187)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6           val_loss 3.9940576553344727\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.8331)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7           val_loss 3.947662830352783\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.7508)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8           val_loss 3.913719415664673\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.6992)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9           val_loss 3.880254030227661\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.6412)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10           val_loss 3.8507142066955566\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.6062)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11           val_loss 3.8096423149108887\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.5552)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12           val_loss 3.76468563079834\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.5162)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13           val_loss 3.7488999366760254\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4952)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14           val_loss 3.742936134338379\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4709)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15           val_loss 3.724591016769409\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4475)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16           val_loss 3.7111124992370605\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4274)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17           val_loss 3.7024776935577393\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4041)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18           val_loss 3.708263635635376\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3954)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19           val_loss 3.693119764328003\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3886)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20           val_loss 3.6809158325195312\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3804)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21           val_loss 3.6841273307800293\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3633)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22           val_loss 3.682408332824707\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3646)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23           val_loss 3.6708970069885254\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3529)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24           val_loss 3.6848747730255127\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3410)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25           val_loss 3.6840949058532715\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3383)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26           val_loss 3.6829986572265625\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3329)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27           val_loss 3.662109851837158\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3221)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28           val_loss 3.6719136238098145\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3117)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29           val_loss 3.6705706119537354\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"np.array(eng_encode[:-500]).shape[0]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T12:57:28.284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ = torch.tensor([x.ids for x in eng_encode[10:30]]).cuda()\noutput_ = torch.tensor([x.ids for x in hin_encode[10:30]]).cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:53.013146Z","iopub.execute_input":"2025-04-22T14:48:53.013585Z","iopub.status.idle":"2025-04-22T14:48:53.020073Z","shell.execute_reply.started":"2025-04-22T14:48:53.013537Z","shell.execute_reply":"2025-04-22T14:48:53.019148Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:53.891775Z","iopub.execute_input":"2025-04-22T14:48:53.892082Z","iopub.status.idle":"2025-04-22T14:48:54.388365Z","shell.execute_reply.started":"2025-04-22T14:48:53.892058Z","shell.execute_reply":"2025-04-22T14:48:54.387724Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"input_.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:54.389399Z","iopub.execute_input":"2025-04-22T14:48:54.389646Z","iopub.status.idle":"2025-04-22T14:48:54.394604Z","shell.execute_reply.started":"2025-04-22T14:48:54.389616Z","shell.execute_reply":"2025-04-22T14:48:54.393939Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"torch.Size([20, 66])"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"en = e.forward(input_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:54.481713Z","iopub.execute_input":"2025-04-22T14:48:54.481982Z","iopub.status.idle":"2025-04-22T14:48:54.487711Z","shell.execute_reply.started":"2025-04-22T14:48:54.481960Z","shell.execute_reply":"2025-04-22T14:48:54.487011Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"hidden = en[1][0][1].unsqueeze(0)\ncell = en[1][1][1].unsqueeze(0)\nen = en[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:55.306910Z","iopub.execute_input":"2025-04-22T14:48:55.307194Z","iopub.status.idle":"2025-04-22T14:48:55.311265Z","shell.execute_reply.started":"2025-04-22T14:48:55.307174Z","shell.execute_reply":"2025-04-22T14:48:55.310613Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"hidden.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:55.567008Z","iopub.execute_input":"2025-04-22T14:48:55.567269Z","iopub.status.idle":"2025-04-22T14:48:55.572254Z","shell.execute_reply.started":"2025-04-22T14:48:55.567251Z","shell.execute_reply":"2025-04-22T14:48:55.571368Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 20, 1000])"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"y_last_pred,hidden, cell = d.forward(output_[:,0].unsqueeze(1), en, hidden,cell)\ny_last_pred = y_last_pred.argmax(dim = 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:55.847033Z","iopub.execute_input":"2025-04-22T14:48:55.847333Z","iopub.status.idle":"2025-04-22T14:48:55.853163Z","shell.execute_reply.started":"2025-04-22T14:48:55.847308Z","shell.execute_reply":"2025-04-22T14:48:55.852373Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"a = torch.zeros((69,20,1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:56.027233Z","iopub.execute_input":"2025-04-22T14:48:56.027528Z","iopub.status.idle":"2025-04-22T14:48:56.031243Z","shell.execute_reply.started":"2025-04-22T14:48:56.027506Z","shell.execute_reply":"2025-04-22T14:48:56.030516Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"a[0] = y_last_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:56.276678Z","iopub.execute_input":"2025-04-22T14:48:56.276968Z","iopub.status.idle":"2025-04-22T14:48:56.280645Z","shell.execute_reply.started":"2025-04-22T14:48:56.276947Z","shell.execute_reply":"2025-04-22T14:48:56.279654Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"for i in range(1,69):\n    y_last_pred,  hidden, cell = d.forward(y_last_pred, en, hidden, cell)\n    y_last_pred = y_last_pred.argmax(dim = 2)\n    a[i] = y_last_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:57.097229Z","iopub.execute_input":"2025-04-22T14:48:57.097506Z","iopub.status.idle":"2025-04-22T14:48:57.244694Z","shell.execute_reply.started":"2025-04-22T14:48:57.097486Z","shell.execute_reply":"2025-04-22T14:48:57.244041Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"a = a.permute(1,0,2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:57.296918Z","iopub.execute_input":"2025-04-22T14:48:57.297173Z","iopub.status.idle":"2025-04-22T14:48:57.300902Z","shell.execute_reply.started":"2025-04-22T14:48:57.297153Z","shell.execute_reply":"2025-04-22T14:48:57.300114Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"a = a.view(20,-1).type(torch.int32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:57.496560Z","iopub.execute_input":"2025-04-22T14:48:57.496850Z","iopub.status.idle":"2025-04-22T14:48:57.501628Z","shell.execute_reply.started":"2025-04-22T14:48:57.496828Z","shell.execute_reply":"2025-04-22T14:48:57.500875Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"hin_tokenizer.decode_batch(a.detach().cpu().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:48:57.726916Z","iopub.execute_input":"2025-04-22T14:48:57.727199Z","iopub.status.idle":"2025-04-22T14:48:57.733103Z","shell.execute_reply.started":"2025-04-22T14:48:57.727180Z","shell.execute_reply":"2025-04-22T14:48:57.732365Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"['एक एक एक एक है है है',\n 'उसने ने ने उसे की की । ।',\n 'मैं मेरे काम से काम काम काम । ।',\n 'मैंने मेरे कल मदद मदद मदद । ।',\n 'अभी अभी तक तक तक तक है ?',\n 'इस अपना अपना अपना । ।',\n 'तुम मैं शादी शादी शादी शादी शादी शादी शादी शादी शादी । ।',\n 'मैं हूँ ।',\n 'मुझे उसके रात रात साथ साथ । ।',\n 'हम वाले वाले वाले ।',\n \"' ' ' ' ' ' ।\",\n 'क्या ! !',\n 'मैं कोई नहीं नहीं नहीं नहीं ।',\n 'वह वह से था !',\n 'तुम बिलकुल नहीं नहीं नहीं ।',\n 'वह मेरा दोस्त है ।',\n 'तुम मेरे सहायक हो ।',\n 'हम अपना काम काम हैं ।',\n 'क्या है है ? ?',\n 'क्या चाय चाय चाय ? ? ?']"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"hin_tokenizer.decode_batch(output_.detach().cpu().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:49:03.462314Z","iopub.execute_input":"2025-04-22T14:49:03.462615Z","iopub.status.idle":"2025-04-22T14:49:03.468651Z","shell.execute_reply.started":"2025-04-22T14:49:03.462589Z","shell.execute_reply":"2025-04-22T14:49:03.467839Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"['देश एक खतरनाक मशीन होती है ।',\n 'भूख ने उसे चोरी करने पर मजबूर किया ।',\n 'मैं अपने काम से ध्यान हटाना चाहता हूँ ।',\n 'कल मैंने मेरे पिता की मदद की ।',\n 'बारिश अभी तक रुकी नहीं है , है ना ?',\n 'अपना मूँह इस तरफ़ मोड़ो ।',\n 'तुम जो भी कहो , मैं तो उससे ही शादी करूँगा ।',\n 'मैं ऊँची हूँ ।',\n 'मेरी उसके साथ आज रात डेट है ।',\n 'हम बात करने वाले हैं ।',\n \"' chat ' इस फ्रांसीसी शब्द का मतलब है ' बिल्ली ' ।\",\n 'लानत है !',\n 'मैं नहीं चाहता कि कोई भी ग़लतफ़ैमी हो ।',\n 'वह एक अनुकरणीय प्रदर्शन था !',\n 'तुम बिलकुल भी बेवकूफ नहीं हो ।',\n 'वह मेरा दोस्त है ।',\n 'आप मेरे नये सहायक हैं ।',\n 'हम तुम्हारा काम बाँट रहें हैं ।',\n 'लिफ़्ट किस तरफ़ है ?',\n 'थोड़ी सी चाय बनाऊं ?']"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"eng_tokenizer.decode_batch(input_.detach().cpu().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:53:08.324588Z","iopub.execute_input":"2025-04-20T08:53:08.324855Z","iopub.status.idle":"2025-04-20T08:53:08.330980Z","shell.execute_reply.started":"2025-04-20T08:53:08.324834Z","shell.execute_reply":"2025-04-20T08:53:08.330017Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"['stay inside .',\n 'bet ?',\n 'this is my japanese friend .',\n \"why isn ' t it here ?\",\n 'tom never tells us anything .',\n 'zulfiqar was the famous sword of hazret - i ali , fourth caliph of islam .',\n 'does arnold schwarzenegger still know german ?',\n 'he watered the rose bush .',\n \"who ' s helping you ?\",\n 'write your name with the pencil .',\n \"i don ' t exist to you .\",\n 'tom most probably forgot .',\n \"tom pulled mary ' s hair .\",\n 'i am afraid of death .',\n 'my mother is a lawyer .',\n \"i ' d caught him red - handed .\",\n 'i found him .',\n \"don ' t expect anything original from an echo .\",\n \"you wouldn ' t understand .\",\n \"i ' ve been studying uighur for two years now .\"]"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"hin_tokenizer.decode_batch(a.detach().cpu().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T09:19:31.074283Z","iopub.execute_input":"2025-04-05T09:19:31.074651Z","iopub.status.idle":"2025-04-05T09:19:31.081369Z","shell.execute_reply.started":"2025-04-05T09:19:31.074623Z","shell.execute_reply":"2025-04-05T09:19:31.080541Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"['अपने पुत्र की प्रशंसा की .',\n 'ग़लती करने से डरता है ।',\n 'वैसे कहने का कोई अधिकार नहीं है ।',\n 'कीजिएगा , पर मैं आपको ठीक से सुन नहीं पा रही हूँ ।',\n 'मेरे नाना हैं ।',\n 'मैं आपको कुछ दिखा सकता हूं ?',\n 'यहाँ उतरना चाहता हूँ .',\n 'अमीर नहीं हैं ।',\n 'लौटने पर क्या तुम यहाँ होगे ?',\n 'अपने पैसे वापस चाहिए ।',\n 'आपकी कहानियों पर विश्वास नहीं है ।',\n 'में तुम कहां बड़ी हुए ?',\n 'ग्राहक ताइवान में हैं , टॉम ने समझाया ।',\n 'है वे हमें जानते हैं ।',\n 'कुछ ज़्यादा ही शोर कर रहे हैं ।',\n 'अपनी मां से प्यार करता हूं ।',\n 'सब एक - जैसे होते हैं !',\n 'कुछ जाये जाये मुझे बता देना ।',\n 'गुदा मैथुन बहुत पसंद है ।',\n 'डाक्टर हो , ना ?']"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"eng_tokenizer.decode_batch(input_.detach().cpu().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T06:59:41.184217Z","iopub.execute_input":"2025-04-05T06:59:41.184541Z","iopub.status.idle":"2025-04-05T06:59:41.190313Z","shell.execute_reply.started":"2025-04-05T06:59:41.184513Z","shell.execute_reply":"2025-04-05T06:59:41.189506Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"['what did he tell you ?',\n 'i wanted tom to study harder .',\n 'is this your child ?',\n \"this is my father ' s house .\",\n 'i am dead to you .',\n 'is it possible to borrow money ?',\n 'she is senior to me by three years .',\n 'ken beat me at chess .',\n 'what language is spoken in mexico ?',\n 'here they are !',\n 'begin !',\n 'you know tom .',\n \"it ' ll only take a minute .\",\n 'i am learning for you .',\n 'he cannot buy a car , still less a house .',\n \"you want answers , don ' t you ?\",\n 'get lost !',\n 'my parents have just arrived at the station .',\n 'you are saying you intentionally hide your good looks ?',\n 'he never takes into account the fact that i am very busy .']"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"q = eng_tokenizer.encode_batch([\"she is going to school by my car\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:33.589023Z","iopub.execute_input":"2025-04-05T07:39:33.589290Z","iopub.status.idle":"2025-04-05T07:39:33.593267Z","shell.execute_reply.started":"2025-04-05T07:39:33.589270Z","shell.execute_reply":"2025-04-05T07:39:33.592398Z"}},"outputs":[],"execution_count":552},{"cell_type":"code","source":"eng_tokenizer.decode(q[0].ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:34.104094Z","iopub.execute_input":"2025-04-05T07:39:34.104390Z","iopub.status.idle":"2025-04-05T07:39:34.109151Z","shell.execute_reply.started":"2025-04-05T07:39:34.104365Z","shell.execute_reply":"2025-04-05T07:39:34.108270Z"}},"outputs":[{"execution_count":553,"output_type":"execute_result","data":{"text/plain":"'she is going to school by my car'"},"metadata":{}}],"execution_count":553},{"cell_type":"code","source":"len(q[0].ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:34.148687Z","iopub.execute_input":"2025-04-05T07:39:34.148916Z","iopub.status.idle":"2025-04-05T07:39:34.153334Z","shell.execute_reply.started":"2025-04-05T07:39:34.148895Z","shell.execute_reply":"2025-04-05T07:39:34.152669Z"}},"outputs":[{"execution_count":554,"output_type":"execute_result","data":{"text/plain":"66"},"metadata":{}}],"execution_count":554},{"cell_type":"code","source":"input_ = torch.tensor([x.ids for x in q[:1]]).cuda()\n# output_ = torch.tensor([x.ids for x in hin_encode[10:30]]).cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:34.218793Z","iopub.execute_input":"2025-04-05T07:39:34.219031Z","iopub.status.idle":"2025-04-05T07:39:34.222965Z","shell.execute_reply.started":"2025-04-05T07:39:34.219012Z","shell.execute_reply":"2025-04-05T07:39:34.222257Z"}},"outputs":[],"execution_count":555},{"cell_type":"code","source":"en = e.forward(input_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:34.258752Z","iopub.execute_input":"2025-04-05T07:39:34.258966Z","iopub.status.idle":"2025-04-05T07:39:34.264071Z","shell.execute_reply.started":"2025-04-05T07:39:34.258948Z","shell.execute_reply":"2025-04-05T07:39:34.263348Z"}},"outputs":[],"execution_count":556},{"cell_type":"code","source":"hidden = en[1][0][1].unsqueeze(0)\ncell = en[1][1][1].unsqueeze(0)\nen = en[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:34.309033Z","iopub.execute_input":"2025-04-05T07:39:34.309285Z","iopub.status.idle":"2025-04-05T07:39:34.316328Z","shell.execute_reply.started":"2025-04-05T07:39:34.309266Z","shell.execute_reply":"2025-04-05T07:39:34.315642Z"}},"outputs":[],"execution_count":557},{"cell_type":"code","source":"hidden.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:34.377210Z","iopub.execute_input":"2025-04-05T07:39:34.377407Z","iopub.status.idle":"2025-04-05T07:39:34.381745Z","shell.execute_reply.started":"2025-04-05T07:39:34.377390Z","shell.execute_reply":"2025-04-05T07:39:34.381132Z"}},"outputs":[{"execution_count":558,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 1, 1000])"},"metadata":{}}],"execution_count":558},{"cell_type":"code","source":"y_last_pred,hidden, cell = d.forward(output_[:1,0].unsqueeze(1), en, hidden,cell)\ny_last_pred = y_last_pred.argmax(dim = 2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:34.433622Z","iopub.execute_input":"2025-04-05T07:39:34.433825Z","iopub.status.idle":"2025-04-05T07:39:34.439015Z","shell.execute_reply.started":"2025-04-05T07:39:34.433807Z","shell.execute_reply":"2025-04-05T07:39:34.438229Z"}},"outputs":[],"execution_count":559},{"cell_type":"code","source":"y_last_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:34.518688Z","iopub.execute_input":"2025-04-05T07:39:34.518877Z","iopub.status.idle":"2025-04-05T07:39:34.523769Z","shell.execute_reply.started":"2025-04-05T07:39:34.518860Z","shell.execute_reply":"2025-04-05T07:39:34.523034Z"}},"outputs":[{"execution_count":560,"output_type":"execute_result","data":{"text/plain":"tensor([[167]], device='cuda:0')"},"metadata":{}}],"execution_count":560},{"cell_type":"code","source":"a = torch.zeros((69,1,1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:35.458998Z","iopub.execute_input":"2025-04-05T07:39:35.459272Z","iopub.status.idle":"2025-04-05T07:39:35.463566Z","shell.execute_reply.started":"2025-04-05T07:39:35.459251Z","shell.execute_reply":"2025-04-05T07:39:35.462544Z"}},"outputs":[],"execution_count":561},{"cell_type":"code","source":"for i in range(0,69):\n    y_last_pred,  hidden, cell = d.forward(y_last_pred, en, hidden, cell)\n    y_last_pred = y_last_pred.argmax(dim = 2)\n    a[i] = y_last_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:35.634216Z","iopub.execute_input":"2025-04-05T07:39:35.634589Z","iopub.status.idle":"2025-04-05T07:39:35.691421Z","shell.execute_reply.started":"2025-04-05T07:39:35.634557Z","shell.execute_reply":"2025-04-05T07:39:35.690802Z"}},"outputs":[],"execution_count":562},{"cell_type":"code","source":"a = a.permute(1,0,2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:35.804000Z","iopub.execute_input":"2025-04-05T07:39:35.804285Z","iopub.status.idle":"2025-04-05T07:39:35.808217Z","shell.execute_reply.started":"2025-04-05T07:39:35.804260Z","shell.execute_reply":"2025-04-05T07:39:35.807242Z"}},"outputs":[],"execution_count":563},{"cell_type":"code","source":"a.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:36.009215Z","iopub.execute_input":"2025-04-05T07:39:36.009508Z","iopub.status.idle":"2025-04-05T07:39:36.014215Z","shell.execute_reply.started":"2025-04-05T07:39:36.009482Z","shell.execute_reply":"2025-04-05T07:39:36.013584Z"}},"outputs":[{"execution_count":564,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 69, 1])"},"metadata":{}}],"execution_count":564},{"cell_type":"code","source":"a = a.view(1,-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:36.169063Z","iopub.execute_input":"2025-04-05T07:39:36.169367Z","iopub.status.idle":"2025-04-05T07:39:36.173589Z","shell.execute_reply.started":"2025-04-05T07:39:36.169341Z","shell.execute_reply":"2025-04-05T07:39:36.172724Z"}},"outputs":[],"execution_count":565},{"cell_type":"code","source":"a = a.type(torch.int32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:36.348771Z","iopub.execute_input":"2025-04-05T07:39:36.349006Z","iopub.status.idle":"2025-04-05T07:39:36.352418Z","shell.execute_reply.started":"2025-04-05T07:39:36.348986Z","shell.execute_reply":"2025-04-05T07:39:36.351790Z"}},"outputs":[],"execution_count":566},{"cell_type":"code","source":"hin_tokenizer.decode_batch(a.detach().cpu().tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:39:36.569097Z","iopub.execute_input":"2025-04-05T07:39:36.569359Z","iopub.status.idle":"2025-04-05T07:39:36.574602Z","shell.execute_reply.started":"2025-04-05T07:39:36.569338Z","shell.execute_reply":"2025-04-05T07:39:36.573881Z"}},"outputs":[{"execution_count":567,"output_type":"execute_result","data":{"text/plain":"['वह मेरी स्कूल से जा रही है ।']"},"metadata":{}}],"execution_count":567},{"cell_type":"code","source":"a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T07:38:50.389401Z","iopub.execute_input":"2025-04-05T07:38:50.389735Z","iopub.status.idle":"2025-04-05T07:38:50.396015Z","shell.execute_reply.started":"2025-04-05T07:38:50.389708Z","shell.execute_reply":"2025-04-05T07:38:50.395144Z"}},"outputs":[{"execution_count":535,"output_type":"execute_result","data":{"text/plain":"tensor([[2481,   10,  172,   10,  172,  156,   10,  156,  150,   21,  131,   21,\n            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n            2,    2,    2,    2,    2,    2,    2,    2,    2]],\n       dtype=torch.int32)"},"metadata":{}}],"execution_count":535},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}