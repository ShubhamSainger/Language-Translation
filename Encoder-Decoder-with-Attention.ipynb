{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11276356,"sourceType":"datasetVersion","datasetId":7049651}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport tokenizers\nimport torch\nfrom torch import nn\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:54.952298Z","iopub.execute_input":"2025-04-22T13:17:54.952587Z","iopub.status.idle":"2025-04-22T13:17:59.239715Z","shell.execute_reply.started":"2025-04-22T13:17:54.952536Z","shell.execute_reply":"2025-04-22T13:17:59.238776Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"data = pd.read_table(\"/kaggle/input/language-translation/Sentence pairs in English-Hindi - 2025-04-02.tsv\", header=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.240860Z","iopub.execute_input":"2025-04-22T13:17:59.241308Z","iopub.status.idle":"2025-04-22T13:17:59.309907Z","shell.execute_reply.started":"2025-04-22T13:17:59.241276Z","shell.execute_reply":"2025-04-22T13:17:59.309022Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\ndata.columns = [\"Eng-ID\",\"Eng\",\"Hin-ID\",\"Hin\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.311883Z","iopub.execute_input":"2025-04-22T13:17:59.312198Z","iopub.status.idle":"2025-04-22T13:17:59.316001Z","shell.execute_reply.started":"2025-04-22T13:17:59.312176Z","shell.execute_reply":"2025-04-22T13:17:59.315217Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data = data.sample(data.shape[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.317457Z","iopub.execute_input":"2025-04-22T13:17:59.317966Z","iopub.status.idle":"2025-04-22T13:17:59.335343Z","shell.execute_reply.started":"2025-04-22T13:17:59.317929Z","shell.execute_reply":"2025-04-22T13:17:59.334791Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"English_data = data.loc[:,[\"Eng-ID\",\"Eng\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.336111Z","iopub.execute_input":"2025-04-22T13:17:59.336402Z","iopub.status.idle":"2025-04-22T13:17:59.355377Z","shell.execute_reply.started":"2025-04-22T13:17:59.336369Z","shell.execute_reply":"2025-04-22T13:17:59.354704Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"Hindi_data = data.loc[:,[\"Hin-ID\",\"Hin\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.356212Z","iopub.execute_input":"2025-04-22T13:17:59.356447Z","iopub.status.idle":"2025-04-22T13:17:59.369941Z","shell.execute_reply.started":"2025-04-22T13:17:59.356427Z","shell.execute_reply":"2025-04-22T13:17:59.369216Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"hin_itr = Hindi_data[\"Hin\"].to_list()\neng_itr = English_data[\"Eng\"].to_list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.370665Z","iopub.execute_input":"2025-04-22T13:17:59.370850Z","iopub.status.idle":"2025-04-22T13:17:59.385049Z","shell.execute_reply.started":"2025-04-22T13:17:59.370833Z","shell.execute_reply":"2025-04-22T13:17:59.384410Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from tokenizers import Tokenizer\n\nfrom tokenizers import normalizers\nfrom tokenizers.normalizers import NFKC, Lowercase\n\nfrom tokenizers import pre_tokenizers\nfrom tokenizers.pre_tokenizers import Whitespace, Punctuation, Digits, BertPreTokenizer\n\nfrom tokenizers.processors import TemplateProcessing\n\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.387153Z","iopub.execute_input":"2025-04-22T13:17:59.387377Z","iopub.status.idle":"2025-04-22T13:17:59.401882Z","shell.execute_reply.started":"2025-04-22T13:17:59.387356Z","shell.execute_reply":"2025-04-22T13:17:59.401214Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n\n\n\nhin_tokenizer = Tokenizer(BPE(unk_token = \"[UNK]\"))\n\nhin_tokenizer.normalizer = normalizers.Sequence([NFKC(), Lowercase()])\n\nhin_tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits(individual_digits=True)])\n\nhin_tokenizer.post_processor = TemplateProcessing(\n    single = \"[SOS] $A [EOS]\",\n    special_tokens = [\n        (\"[SOS]\",1),\n        (\"[EOS]\",2),\n        (\"[PAD]\",0)\n    ]\n)\n\ntrainer = BpeTrainer( special_tokens = [\"[PAD]\", \"[SOS]\" , \"[EOS]\", \"[UNK]\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.403315Z","iopub.execute_input":"2025-04-22T13:17:59.403554Z","iopub.status.idle":"2025-04-22T13:17:59.422754Z","shell.execute_reply.started":"2025-04-22T13:17:59.403535Z","shell.execute_reply":"2025-04-22T13:17:59.421817Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"hin_tokenizer.train_from_iterator(hin_itr,trainer)\n\nhin_tokenizer.enable_padding()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.423534Z","iopub.execute_input":"2025-04-22T13:17:59.423841Z","iopub.status.idle":"2025-04-22T13:17:59.790448Z","shell.execute_reply.started":"2025-04-22T13:17:59.423814Z","shell.execute_reply":"2025-04-22T13:17:59.789782Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"hin_tokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.791152Z","iopub.execute_input":"2025-04-22T13:17:59.791425Z","iopub.status.idle":"2025-04-22T13:17:59.800460Z","shell.execute_reply.started":"2025-04-22T13:17:59.791389Z","shell.execute_reply":"2025-04-22T13:17:59.799688Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"10463"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"hin_encode = hin_tokenizer.encode_batch(hin_itr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:17:59.801398Z","iopub.execute_input":"2025-04-22T13:17:59.801770Z","iopub.status.idle":"2025-04-22T13:18:00.179322Z","shell.execute_reply.started":"2025-04-22T13:17:59.801714Z","shell.execute_reply":"2025-04-22T13:18:00.178648Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"hin_vocab_len = hin_tokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.180069Z","iopub.execute_input":"2025-04-22T13:18:00.180349Z","iopub.status.idle":"2025-04-22T13:18:00.186668Z","shell.execute_reply.started":"2025-04-22T13:18:00.180321Z","shell.execute_reply":"2025-04-22T13:18:00.185813Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"eng_tokenizer = Tokenizer(BPE(unk_token = \"[UNK]\"))\n\neng_tokenizer.normalizer = normalizers.Sequence([NFKC(), Lowercase()])\n\neng_tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits(individual_digits=True)])\n\neng_tokenizer.post_processor = TemplateProcessing(\n    single = \"[SOS] $A [EOS]\",\n    special_tokens = [\n        (\"[SOS]\",1),\n        (\"[EOS]\",2),\n        (\"[PAD]\",0)\n    ]\n)\n\ntrainer = BpeTrainer( special_tokens = [\"[PAD]\", \"[SOS]\" , \"[EOS]\", \"[UNK]\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.187803Z","iopub.execute_input":"2025-04-22T13:18:00.188103Z","iopub.status.idle":"2025-04-22T13:18:00.202311Z","shell.execute_reply.started":"2025-04-22T13:18:00.188070Z","shell.execute_reply":"2025-04-22T13:18:00.201660Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"eng_tokenizer.train_from_iterator(eng_itr,trainer)\neng_tokenizer.enable_padding()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.203199Z","iopub.execute_input":"2025-04-22T13:18:00.203486Z","iopub.status.idle":"2025-04-22T13:18:00.498826Z","shell.execute_reply.started":"2025-04-22T13:18:00.203464Z","shell.execute_reply":"2025-04-22T13:18:00.497902Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"eng_vocab_len = eng_tokenizer.get_vocab_size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.499745Z","iopub.execute_input":"2025-04-22T13:18:00.500036Z","iopub.status.idle":"2025-04-22T13:18:00.506314Z","shell.execute_reply.started":"2025-04-22T13:18:00.500010Z","shell.execute_reply":"2025-04-22T13:18:00.505474Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"eng_encode = eng_tokenizer.encode_batch(eng_itr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.507225Z","iopub.execute_input":"2025-04-22T13:18:00.507434Z","iopub.status.idle":"2025-04-22T13:18:00.843759Z","shell.execute_reply.started":"2025-04-22T13:18:00.507416Z","shell.execute_reply":"2025-04-22T13:18:00.842699Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"hind_padded_size = len(hin_encode[0])\neng_padded_size = len(eng_encode[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.844558Z","iopub.execute_input":"2025-04-22T13:18:00.845125Z","iopub.status.idle":"2025-04-22T13:18:00.851360Z","shell.execute_reply.started":"2025-04-22T13:18:00.845030Z","shell.execute_reply":"2025-04-22T13:18:00.850232Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"training_size = int(data.shape[0] * 0.7)\n# testing_size = data.shape[0] - training_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.852475Z","iopub.execute_input":"2025-04-22T13:18:00.852822Z","iopub.status.idle":"2025-04-22T13:18:00.868823Z","shell.execute_reply.started":"2025-04-22T13:18:00.852789Z","shell.execute_reply":"2025-04-22T13:18:00.868142Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"shuffeled_index = np.random.permutation(np.arange(0,data.shape[0]))\ntraining_eng_data = np.array(eng_encode)[shuffeled_index[:training_size]]\ntraining_hin_data = np.array(hin_encode)[shuffeled_index[:training_size]]\n\n\ntesting_eng_data = np.array(eng_encode)[shuffeled_index[training_size:]]\ntesting_hin_data = np.array(hin_encode)[shuffeled_index[training_size:]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.869555Z","iopub.execute_input":"2025-04-22T13:18:00.869838Z","iopub.status.idle":"2025-04-22T13:18:00.960246Z","shell.execute_reply.started":"2025-04-22T13:18:00.869818Z","shell.execute_reply":"2025-04-22T13:18:00.959516Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eng_tokenizer.decode_batch([x.ids for x in testing_eng_data[:10]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.961026Z","iopub.execute_input":"2025-04-22T13:18:00.961254Z","iopub.status.idle":"2025-04-22T13:18:00.967243Z","shell.execute_reply.started":"2025-04-22T13:18:00.961235Z","shell.execute_reply":"2025-04-22T13:18:00.966630Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['is that a cat ?',\n \"they ' re using you .\",\n 'he likes playing soccer .',\n 'they dug here and there for treasure .',\n \"you ' re doing very well . keep it up .\",\n 'she has lived there for seven years .',\n \"you often ask questions i can ' t answer .\",\n 'we were worried about you .',\n 'let me know if you find anything .',\n 'just wait a minute .']"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"hin_tokenizer.decode_batch([x.ids for x in testing_hin_data[:10]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.968047Z","iopub.execute_input":"2025-04-22T13:18:00.968291Z","iopub.status.idle":"2025-04-22T13:18:00.984973Z","shell.execute_reply.started":"2025-04-22T13:18:00.968272Z","shell.execute_reply":"2025-04-22T13:18:00.984162Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['वह बिल्ली है क्या ?',\n 'वे तुम्हारा इस्तेमाल कर रहे हैं ।',\n 'उसको फ़ुटबॉल खेलना अच्छा लगता है ।',\n 'उन्होंने ख़ज़ाना ढूँढने के लिए यहाँ - वहाँ खोदा ।',\n 'तुम अच्छा काम कर रहे हो । ऐसे ही करते रहो ।',\n 'वह वहाँ सात साल रही है ।',\n 'आप अकसर ऐसे सवाल पूछते हैं जिनका मैं जवाब नहीं दे सकती ।',\n 'हमें आपकी चिंता लगी हुई है ।',\n 'कुछ मिल जाये तो मुझे बता देना ।',\n 'बस एक मिनट रुक ।']"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# Class to create a encoder which will take input sequence and give the hidden states at each time step\n\nclass encoder(nn.Module):\n    def __init__(self):        \n        super().__init__()\n        self.embedding_layer = nn.Embedding(eng_vocab_len, 620)\n        self.rnn1 = nn.LSTM(620,1000, batch_first =True, bidirectional = True)\n        self.dropout = nn.Dropout(p = 0.3) \n\n    def forward(self,input_batch):\n        x = self.embedding_layer(input_batch)\n        x, (h,c) = self.rnn1(x)\n        x = self.dropout(x)\n        return (x, (h,c))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:00.988641Z","iopub.execute_input":"2025-04-22T13:18:00.988857Z","iopub.status.idle":"2025-04-22T13:18:01.000308Z","shell.execute_reply.started":"2025-04-22T13:18:00.988838Z","shell.execute_reply":"2025-04-22T13:18:00.999595Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Class to calculate the context vector for the current time step by taking the hidden state of the decoder and all the hidden states given by the \n# encoder\n\nclass attention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.dense_1 = nn.Linear(2000,1000)\n        self.dense_2 = nn.Linear(1000,1000)\n        self.dense_3 = nn.Linear(1000,1)\n\n    def forward(self, decoder_hidden_state, encoder_hidden_state):\n\n        x = self.dense_1(encoder_hidden_state)\n        y = self.dense_2(decoder_hidden_state).permute(1,0,2)\n        x = torch.add(x,y)\n        x = nn.functional.tanh(x)\n        x = self.dense_3(x)\n        x = nn.functional.softmax(x, dim = 1)\n        x  = torch.sum(x * encoder_hidden_state, dim = 1)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:01.001673Z","iopub.execute_input":"2025-04-22T13:18:01.001978Z","iopub.status.idle":"2025-04-22T13:18:01.016834Z","shell.execute_reply.started":"2025-04-22T13:18:01.001946Z","shell.execute_reply":"2025-04-22T13:18:01.015656Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# This is the maxout layer explained in the paper and it will project the data in two different vector spaces of same dimension and \n# then calculate the maximum value of a element from both the dimensions.\n\nclass maxout(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.Dense_1 = nn.Linear(1000,hin_vocab_len)\n        self.Dense_2 = nn.Linear(1000,hin_vocab_len)\n\n    def forward(self,x):\n        lp_1 = self.Dense_1(x)\n        lp_2 = self.Dense_2(x)\n        lp_1 = torch.cat([lp_1,lp_2], dim = 1)\n        lp_1 = lp_1.max(dim = 1)[0].unsqueeze(1)\n        return lp_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:01.017847Z","iopub.execute_input":"2025-04-22T13:18:01.018127Z","iopub.status.idle":"2025-04-22T13:18:01.036791Z","shell.execute_reply.started":"2025-04-22T13:18:01.018096Z","shell.execute_reply":"2025-04-22T13:18:01.036010Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Class to create a decoder which will take previous prediction, hidden states given by encoder at each time step,\n# it's own previous hidden states and cell states\n\nclass decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedding = nn.Embedding(hin_vocab_len,620, padding_idx=0)\n        self.norm_1 = nn.LayerNorm(620 + 2000)\n        self.rnn = nn.LSTM(620 + 2000,1000,batch_first =True)\n        self.norm_2 = nn.LayerNorm(1000)\n        self.dropout= nn.Dropout(p = 0.3)\n        # self.dense = nn.Linear(800,hin_vocab_len)\n        self.maxout = maxout()\n        self.attention = attention()\n\n\n    def forward(self, y_last_pred, encoder_hidden_states, decoder_hidden_state, cell_hidden_state):\n\n        y_last_pred = self.embedding(y_last_pred)\n        context_vector = self.attention(decoder_hidden_state, encoder_hidden_states).unsqueeze(1)\n        y_last_pred = torch.cat((context_vector,y_last_pred),dim=2)\n        y_last_pred = self.norm_1(y_last_pred)\n        x , (hidden, cell) = self.rnn(y_last_pred,(decoder_hidden_state,cell_hidden_state))\n        x = self.norm_2(x)\n        x = self.dropout(x)\n        # x = self.dense(x)\n        x = self.maxout(x)\n        return x, hidden,cell\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:01.037523Z","iopub.execute_input":"2025-04-22T13:18:01.037843Z","iopub.status.idle":"2025-04-22T13:18:01.046281Z","shell.execute_reply.started":"2025-04-22T13:18:01.037814Z","shell.execute_reply":"2025-04-22T13:18:01.045592Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"e = encoder().cuda()\nd = decoder().cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:01.047044Z","iopub.execute_input":"2025-04-22T13:18:01.047276Z","iopub.status.idle":"2025-04-22T13:18:02.015809Z","shell.execute_reply.started":"2025-04-22T13:18:01.047258Z","shell.execute_reply":"2025-04-22T13:18:02.015081Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"loss = nn.CrossEntropyLoss(ignore_index = 0 ).cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:02.016625Z","iopub.execute_input":"2025-04-22T13:18:02.016919Z","iopub.status.idle":"2025-04-22T13:18:02.021389Z","shell.execute_reply.started":"2025-04-22T13:18:02.016892Z","shell.execute_reply":"2025-04-22T13:18:02.020771Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"encoder_bias_parm = []\nencoder_thetha = []\nfor name, weight in list(e.named_parameters()):\n    if \"bias\" in name:\n        encoder_bias_parm.append(weight)\n\n    else:\n        encoder_thetha.append(weight)\n\n\ndecoder_bias_parm = []\ndecoder_theta = []\nfor name, weight in list(d.named_parameters()):\n    if \"bias\" in name:\n        decoder_bias_parm.append(weight)\n\n    else:\n        decoder_theta.append(weight)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:02.038698Z","iopub.execute_input":"2025-04-22T13:18:02.039046Z","iopub.status.idle":"2025-04-22T13:18:02.053933Z","shell.execute_reply.started":"2025-04-22T13:18:02.039014Z","shell.execute_reply":"2025-04-22T13:18:02.053219Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Applying regularization only on parameter vectors not on the bias terms\n\noptim = torch.optim.Adam([{\"params\":encoder_bias_parm + decoder_bias_parm, \"weight_decay\":0},\n                        {\"params\": encoder_thetha + decoder_theta}], weight_decay = 0.005, lr = 0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:02.069710Z","iopub.execute_input":"2025-04-22T13:18:02.069894Z","iopub.status.idle":"2025-04-22T13:18:03.859175Z","shell.execute_reply.started":"2025-04-22T13:18:02.069878Z","shell.execute_reply":"2025-04-22T13:18:03.858486Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Main training loop\n\ndef lang_training(orignal_sequence, target_sequence,val_orignal_sequence, val_target_sequence, batch_size, epochs, encoder, decoder, target_vocab_len,):\n\n    \n    total_sequence = (len(orignal_sequence)//batch_size) * batch_size\n    taget_sequence_length = torch.tensor(orignal_sequence[0].ids).shape[0]\n    average_training_loss = {}\n    orignal_sequence = np.array(orignal_sequence)\n    target_sequence = np.array(target_sequence)\n    \n    for epoch in range(1,epochs):\n        encoder.train()\n        decoder.train()\n        random_index = np.random.permutation(np.arange(0,orignal_sequence.shape[0]))\n        orignal_sequence = orignal_sequence[random_index]\n        target_sequence = target_sequence[random_index]\n        loss_per_batch = []\n        with tqdm(range(0,total_sequence, batch_size)) as tq:\n            for time_step, idx in enumerate(tq):\n        \n                \n                # Collecting inputs and targets.\n                input_ = torch.tensor([x.ids for x in orignal_sequence[idx:idx + batch_size]]).cuda()                \n                output_ = torch.tensor([x.ids for x in target_sequence[idx:idx + batch_size]]).cuda()\n        \n                # Extracting all the hidden states and the cell states from the encoder, using the [SOS] token for a batch.\n                en_out = encoder.forward(input_)\n                \n                hidden = en_out[1][0][1].unsqueeze(0) \n                cell = en_out[1][1][1].unsqueeze(0)\n    \n                en_out = en_out[0]\n            \n                # Assigning 2 np arrays for storing the probabilites of each token and the target token respectively in each timestep.\n                y_p_loss = torch.zeros(68,batch_size,target_vocab_len).cuda()\n                y_t_loss = torch.zeros(68,batch_size).cuda()\n        \n            \n                # Using the final hidden state of the encoder as the initial hidden state of the decoder and generating the y_1_hat \n                y_last_pred, hidden, cell = decoder.forward(output_[:,0].unsqueeze(1), en_out, hidden, cell)\n         \n                y_true = output_[:,1] # Loss will be calculated on target[t+1] index\n                l = 0\n        \n                y_p_loss[0] = y_last_pred.squeeze(1)\n                y_t_loss[0] = y_true\n                \n                # Finding the index of the token which have the max probability\n                y_last_pred = y_last_pred.argmax(dim = 2)\n                \n        \n                # Generating the probabilites of the tokens for further timesteps and storing them in the assigined variables\n                for i in range(1,68):\n                    \n                    y_last_pred,  hidden, cell = decoder.forward(y_last_pred, en_out, hidden, cell)\n                    y_true = output_[:,i+1]            \n                    y_p_loss[i] = y_last_pred.squeeze(1)\n                    y_t_loss[i] = y_true\n                    \n                    y_last_pred = y_last_pred.argmax(dim = 2)\n        \n                # Reshaping the tensors to match the requirement of the CCE loss -> (Batchsize * sequence, classes)\n                y_p_loss=y_p_loss.view(-1, target_vocab_len)\n                y_t_loss=y_t_loss.view(-1).type(torch.long)\n            \n                l = loss(y_p_loss, y_t_loss)\n                optim.zero_grad()\n                l.backward()\n                optim.step()\n                loss_per_batch.append(l.detach().cpu())\n                tq.set_postfix({\"Loss\":torch.tensor(loss_per_batch).mean()})\n\n        # average_training_loss[epoch] = torch.tensor(loss_per_batch).mean()\n        # print(f\"Epoch {epoch}           training_loss {average_training_loss[epoch]} \")\n        \n        val_loss = validation_performance(val_orignal_sequence, val_target_sequence, 50, e,d, target_vocab_len)\n        print(f\"Epoch {epoch}           val_loss {val_loss}\")\n    return average_training_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:03.859898Z","iopub.execute_input":"2025-04-22T13:18:03.860222Z","iopub.status.idle":"2025-04-22T13:18:03.870278Z","shell.execute_reply.started":"2025-04-22T13:18:03.860202Z","shell.execute_reply":"2025-04-22T13:18:03.869421Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Slight modification of main training loop to work on validation data\n\ndef validation_performance(orignal_sequence,target_sequence,batch_size,encoder,decoder,target_vocab_len):\n    encoder.eval()\n    decoder.eval()\n    total_sequence = (len(orignal_sequence)//batch_size) * batch_size\n    taget_sequence_length = torch.tensor(orignal_sequence[0].ids).shape[0]\n    loss_per_batch = []\n    with torch.no_grad():\n        for time_step, idx in enumerate(range(0,total_sequence, batch_size)):\n        \n            # Collecting inputs and targets.\n            input_ = torch.tensor([x.ids for x in orignal_sequence[idx:idx + batch_size]]).cuda()                \n            output_ = torch.tensor([x.ids for x in target_sequence[idx:idx + batch_size]]).cuda()\n        \n            # Extracting all the hidden states and the cell states from the encoder, using the [SOS] token for a batch.\n            en_out = encoder.forward(input_)\n            \n            hidden = en_out[1][0][1].unsqueeze(0) \n            cell = en_out[1][1][1].unsqueeze(0)\n        \n            en_out = en_out[0]\n        \n            # Assigning 2 np arrays for storing the probabilites of each token and the target token respectively in each timestep.\n            y_p_loss = torch.zeros(68,batch_size,target_vocab_len).cuda()\n            y_t_loss = torch.zeros(68,batch_size).cuda()\n        \n        \n            # Using the final hidden state of the encoder as the initial hidden state of the decoder and generating the y_1_hat \n            y_last_pred, hidden, cell = decoder.forward(output_[:,0].unsqueeze(1), en_out, hidden, cell)\n        \n            y_true = output_[:,1] # Loss will be calculated on target[t+1] index\n            l = 0\n            y_p_loss[0] = y_last_pred.squeeze(1)\n            y_t_loss[0] = y_true\n            \n            # Finding the index of the token which have the max probability\n            y_last_pred = y_last_pred.argmax(dim = 2)\n            \n        \n            # Generating the probabilites of the tokens for further timesteps and storing them in the assigined variables\n            for i in range(1,68):\n                \n                y_last_pred,  hidden, cell = decoder.forward(y_last_pred, en_out, hidden, cell)\n                y_true = output_[:,i+1]            \n                y_p_loss[i] = y_last_pred.squeeze(1)\n                y_t_loss[i] = y_true\n                \n                y_last_pred = y_last_pred.argmax(dim = 2)\n        \n            # Reshaping the tensors to match the requirement of the CCE loss -> (Batchsize * sequence, classes)\n            y_p_loss=y_p_loss.view(-1, target_vocab_len)\n            y_t_loss=y_t_loss.view(-1).type(torch.long)\n        \n            l = loss(y_p_loss, y_t_loss)\n            loss_per_batch.append(l.detach().cpu())\n\n    return torch.tensor(loss_per_batch).mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:03.899929Z","iopub.execute_input":"2025-04-22T13:18:03.900196Z","iopub.status.idle":"2025-04-22T13:18:03.914684Z","shell.execute_reply.started":"2025-04-22T13:18:03.900164Z","shell.execute_reply":"2025-04-22T13:18:03.913848Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"\nlang_training(eng_encode[:-1500], hin_encode[:-1500],eng_encode[-1500:],hin_encode[-1500:], 180, 30, e,d, hin_vocab_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:18:03.915663Z","iopub.execute_input":"2025-04-22T13:18:03.915938Z","iopub.status.idle":"2025-04-22T14:48:25.849722Z","shell.execute_reply.started":"2025-04-22T13:18:03.915918Z","shell.execute_reply":"2025-04-22T14:48:25.848872Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 64/64 [02:57<00:00,  2.77s/it, Loss=tensor(5.6287)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1           val_loss 4.960709095001221\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(4.7877)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2           val_loss 4.5693159103393555\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(4.4330)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3           val_loss 4.369600296020508\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(4.2107)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4           val_loss 4.193140029907227\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(4.0409)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5           val_loss 4.084902763366699\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.9187)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6           val_loss 3.9940576553344727\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.8331)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7           val_loss 3.947662830352783\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.7508)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8           val_loss 3.913719415664673\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.6992)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9           val_loss 3.880254030227661\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.6412)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10           val_loss 3.8507142066955566\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.6062)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11           val_loss 3.8096423149108887\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.5552)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12           val_loss 3.76468563079834\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.5162)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13           val_loss 3.7488999366760254\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4952)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14           val_loss 3.742936134338379\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4709)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15           val_loss 3.724591016769409\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4475)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16           val_loss 3.7111124992370605\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4274)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17           val_loss 3.7024776935577393\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.4041)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18           val_loss 3.708263635635376\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3954)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19           val_loss 3.693119764328003\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3886)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20           val_loss 3.6809158325195312\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3804)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21           val_loss 3.6841273307800293\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3633)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22           val_loss 3.682408332824707\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3646)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23           val_loss 3.6708970069885254\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3529)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24           val_loss 3.6848747730255127\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3410)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25           val_loss 3.6840949058532715\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3383)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26           val_loss 3.6829986572265625\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3329)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27           val_loss 3.662109851837158\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3221)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28           val_loss 3.6719136238098145\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [02:56<00:00,  2.76s/it, Loss=tensor(3.3117)]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29           val_loss 3.6705706119537354\n","output_type":"stream"}],"execution_count":36}]}